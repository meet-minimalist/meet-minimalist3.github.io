---
title: 'Qwen paper summary'
date: 2025-11-09
permalink: /posts/2025/11/qwen_paper/
tags:
  - LLM
  - Open Source
---

### Summary
- Qwen model trained upto 3T tokens on the new dataset they developed.
- BPE tokenizer (tiktoken) enhanced for Chinese character. Vocab size 152k.
- Architecture derived from Llama2.
	- Untied embedding and projection layer.
	- FP32 for inverse frequency matrix of RoPE instead of BF16/FP16 for model performance and higher accuracy.
	- Bias removed from most of the layers but retained in QKV to enhance the extrapolation abilities.
	- PreNorm (instead of PostNorm)	and RMSNorm (instead of LayerNorm)
	- SwiGLU over GeLU
	- FFN upsample to 8/3 dimensionality instead of 4x.
- Pretraining
	- Autoregressive training with context length of 2048 with a batch size of 2048 such that 4M tokens are trained in each batch.
	- Flash attention
	- AdamW (beta_1 = 0.9, beta_2 = 0.95, eps = 1e-8)
	- Cosine LR, min LR = 10% of peak LR.
	- BF16 mixed precision training for stability.
	- 1.8B trained for 2.2T
	- 7B trained for 2.4T
	- 14B trained for 3.0T
- Context length extension at inference
	- NTK aware interpolation which adjusts base of RoPE to prevent the loss of high frequency information in training free manner. (another similar technique to check is position interpolation which scales each dimension of RoPE equally)
	- LogN scaling in attention which rescales the dot product of query and value by a factor that depends on ratio of context length to the training length, ensuring that the entropy of the attention value remains stable as context length grows.
	- Window attention to limit the context window. Lower layers (layers near to input) are more sensitive to context length extension compared to higher layers. Hence, using shorter windows for lower layers and longer windows for higher layers.
- Post training
	- For aligning models to human like responses, they developed their own version of dataset for instructions, questions and answers which does not include specific prompt templates. They used ChatML style format which helps in distinguishing between system steup, user input and assistant outputs.
	- During SFT, loss masks system and user inputs and trained on assistant outputs.
		- AdamW (beta_1 = 0.9, beta_2 = 0.95, eps = 1e-8)
		- Context length of 2048 and batch size of 128 which is 262144 tokens in a batch.
		- Total 4000 steps means 1,048,576,000 ~ 1B tokens, which is ~0.1% of pretrained tokens
		- SFT LR is of 1e-6 scale and also used gradient clipping to 1.0
	- For RLHF, they used reward model and PPO to conduct policy training.
		- Reward modeling
			- Preference dataset consists of a prompt and pair of responses out of which one is more preferred by human annotator.
			- They developed this by using user prompts and generating different responses using qwen models of different sizes and sampling strategies. These responses are evaluated by annotator to form a proper label.
			- Reward model and Policy model are of same size.
			- It is important to mention that we have incorporated a pooling layer into the original QWEN model to extract the reward for a sentence based on a specific end token. 
			- Reward training LR is of 1e-6 scale, 2048 context length and 64 bs ~ 131072 tokens per batch. Only one epoch training.
		- PPO process uses 4 models. Policy model, Value model, Reference model and Reward model.
			- Before starting the PPO procedure, we pause the policy modelâ€™s updates and focus solely on updating the value model for 50 steps. This approach ensures that the value model can adapt to different reward models effectively.
			- Need to re-read this section after learning about PPO in detail and the interplay between 4 models.
- Evaluation
	- For evaluation of aligned model, they are comparing their models with zero shot setting instead of few shot setting for well estabilished benchmarks like MMLU, C-Eval, GSM8k, HumanEval, BBH. The aligned model with zero shot setting works equivalent to few shot setting model.
	- Evaluation on inhouse dataset of 300 chinese samples, compared independently against GPT3.5 for win/ties and losses.
	- For tool caling they have open sourced a benchmark. The assessment will be based on the fact that llm selects correct plugin from pool of up to five candidates and correctness of parameters passed. The model fits benchmark well as it goes to 14B, that suggests that it is an easy one and need to explore difficult benchmark.
	- For code models and math models, they have open sourced the benchmarks. These has easy and challenging tasks. Easy being one steps task whereas challenging being multistep tasks each depending on the answer of previous steps.
- Qwen code
	- Earlier models, were using only code data for the pretraining of code version of llm. But qwen starts with the base model trained on text and code data then continues to pretrain on code data. The continued pretraining happens for 90B tokens ~ 0.09T tokens which is 3% of the pretraining tokens.
	- Context length of 8192
	- LR of scale 1e-5, with 3% warm up iterations with no lr-decay.
	- Code SFT is performed over the base Code variant of the model.
		- LR of scale 1e-6 to 1e-5.
	- Evaluation on MBPP and HumanEval benchmarks and HumanEvalPack benchmark
- Qwen math
	- Qwen base used as base model and SFT is performed using math dataset to get Math-Qwen-Chat
	- 1024 context length
	- 1e-5 lr scale and trained for 50,000 steps
	- Evaluation on GSM8k (grade school math), MATH (challenging competition math), Math401 (arithmetic abilities), Math23K (chinese grade school math)


### Questions:
- How compression ratio is computed while comparing different tokenizers on different languages?
	Higher compression means, lesser tokens required when tokenizing the text. 
	A english trained Llama model if directly used for legal or medical purposes, then there will be many words which are not present in the vocab and they can be splitted during tokenization unnecessarily. That will increase the number of tokens when tokenizing. Instead in those kind of cases we either should use new tokenizer or extend the existing tokenizer's vocab size.
- What is alignment tax in RLHF training?

### Checkout
- Preference model training (Bai et al. 2022b) paper
- Need to re-read this section after learning about PPO in detail and the interplay between 4 models.
- What is ReAct prompting (Yao et. al. 2022) paper?


